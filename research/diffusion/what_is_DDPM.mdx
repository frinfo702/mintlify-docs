---
title: "What are Diffusion Models?"
description: "An approachable explanation of denoising diffusion probabilistic models (DDPMs)."
---

Diffusion models generate data by gradually destroying structure with noise and then learning how to reverse that process. The modern formulation—denoising diffusion probabilistic models (DDPMs)—was popularized by Ho et al. (2020) and now powers tools from image generators to speech enhancers.

## Why diffusion?

<Columns cols={2}>
  <Card title="Stable training" icon="gauge">
  Training reduces to predicting noise with mean-squared error, avoiding the mode collapse seen in GANs.
  </Card>
  <Card title="Likelihood access" icon="chart-simple">
  DDPMs optimize a variational bound, so you can estimate log-likelihoods—unlike most implicit generators.
  </Card>
</Columns>

## Core idea

1. **Forward process**: add a tiny amount of Gaussian noise for `T` steps until the original sample becomes nearly pure noise.
2. **Reverse process**: train a neural network to predict the noise that was added at each step, allowing you to denoise iteratively and recover a clean sample starting from random noise.

### Forward (noising) process

We define a variance schedule with small positive values beta_1, ..., beta_T. Given a real sample x_0:

<Latex>{String.raw`q(\mathbf{x}_t \,|\, \\mathbf{x}_{t-1}) = \\mathcal{N}\left(\sqrt{1-\beta_t}\,\mathbf{x}_{t-1},\; \beta_t \mathbf{I}\right)`}</Latex>

Thanks to Gaussian composition, you can sample any timestep directly:

<Latex>{String.raw`\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})`}</Latex>

Here alpha_t = 1 - beta_t and alpha_bar_t multiplies alpha_s for every s up through t.

### Reverse (denoising) process

We cannot compute the exact reverse conditional q(x_{t-1} | x_t) analytically, so we train a neural network epsilon_theta(x_t, t) to predict the noise that was injected at each step.

<Steps>
  <Step title="Sample noise">
    Draw t ~ Uniform(1, ..., T) and noise epsilon ~ N(0, I).
  </Step>
  <Step title="Corrupt the data">
    Produce x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon.
  </Step>
  <Step title="Predict the noise">
    Optimize the mean squared error || epsilon - epsilon_theta(x_t, t) ||^2.
  </Step>
</Steps>

Once trained, start from x_T ~ N(0, I) and iteratively apply the learned reverse transitions to sample fresh data.

## Training objective

The ELBO derived in the original DDPM paper collapses to a simple noise-prediction loss:

<Latex>{String.raw`\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}}\left[\left\| \boldsymbol{\epsilon} - \epsilon_\theta\left(\sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon},\; t\right) \right\|_2^2\right]`}</Latex>

Why it works:

- The Gaussian structure makes every KL term in the ELBO analytically tractable.
- Matching predicted noise to the true noise is equivalent to matching the reverse means.
- Weighting timesteps by `t` (for example, cosine or linear schedules) encourages the network to handle both high- and low-noise regimes.

## Sampling loop (pseudo-code)

```python
x_t = torch.randn_like(x0)
for t in reversed(range(1, T + 1)):
    eps_theta = model(x_t, t)
    mean = (1 / math.sqrt(alpha_t)) * (x_t - beta_t / math.sqrt(1 - alpha_bar_t) * eps_theta)
    if t > 1:
        noise = torch.randn_like(x_t)
    else:
        noise = 0
    x_t = mean + math.sqrt(beta_t_tilde) * noise
return x_t
```

<Tip>`beta_t_tilde` is the posterior variance term derived from the forward process; implementations usually clamp it for numerical stability.</Tip>

## Implementation checklist

<Checklist>
  <CheckItem>Normalize training data to `[-1, 1]` so the Gaussian assumptions hold.</CheckItem>
  <CheckItem>Encode the timestep with sinusoidal embeddings (same as Transformers) and feed it into the UNet backbone.</CheckItem>
  <CheckItem>Use classifier-free guidance or conditioning embeddings to steer the sampler toward desired attributes.</CheckItem>
  <CheckItem>During inference, switch to improved samplers (DDIM, DPM++) for faster generation once the base model is trained.</CheckItem>
</Checklist>

## Further reading

- Ho et al., 2020. *Denoising Diffusion Probabilistic Models.*
- Song et al., 2021. *Score-Based Generative Modeling through Stochastic Differential Equations.*
- Nichol & Dhariwal, 2021. *Improved Denoising Diffusion Probabilistic Models.*
